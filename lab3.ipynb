{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3\n",
    "*Introduction have been remove to gain place*\n",
    "### Analyzing the network\n",
    "\n",
    "Many details of the network are currently hidden as default parameters.\n",
    "\n",
    "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
    "\n",
    "- What is the structure of the network?  \n",
    "<span style=\"color:blue\">It is a neural network.</span>\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?  \n",
    "<span style=\"color:blue\">The default algorithm is 'adam'. The stochastic gradient descent (SGD) algorithm that we studied during the courses is also available.</span>\n",
    "- How does the training algorithm decides to stop the training?  \n",
    "<span style=\"color:blue\">It stops when the maximum number of iterations is reached. If the early stoping is activated, it can also stop when no improvment is done to the validation score.</span>3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onto a more challenging dataset: house prices\n",
    "\n",
    "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all previously defined variables for the sailing boats\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.0368</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>1.103627</td>\n",
       "      <td>413.0</td>\n",
       "      <td>2.139896</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.6591</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>2.128405</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.1200</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.788253</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.6912</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.172269</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "5  4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
       "6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   \n",
       "7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
       "8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   \n",
       "9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  \n",
       "5    -122.25  \n",
       "6    -122.25  \n",
       "7    -122.25  \n",
       "8    -122.26  \n",
       "9    -122.25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "num_samples = 2000 # Only use the first N samples to limit training time\n",
    "cal_housing = fetch_california_housing()\n",
    "X = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)[:num_samples]\n",
    "y = cal_housing.target[:num_samples]\n",
    "X.head(10) # print the first 10 values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a subpart of the dataset for testing\n",
    "\n",
    "- Split the dataset between a training set (75%) and a test set (25%)\n",
    "\n",
    "Please use the conventional names `X_train`, `X_test`, `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the input data\n",
    "\n",
    "\n",
    "A step of **scaling** of the data is often useful to ensure that all input data centered on 0 and with a fixed variance.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). The function `StandardScaler` from `sklearn.preprocessing` computes the standard score of a sample as:\n",
    "\n",
    "```\n",
    "z = (x - u) / s\n",
    "```\n",
    "\n",
    "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\n",
    "\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using transform.\n",
    "\n",
    " - Apply the standard scaler to both the training dataset (`X_train`) and the test dataset (`X_test`).\n",
    " - Make sure that **exactly the same transformation** is applied to both datasets.\n",
    "\n",
    "[Documentation of standard scaler in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import model_selection\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "#We split the model in three with a train set, a test set and a validation set\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "X_test, X_validation, y_test, y_validation = model_selection.train_test_split(X_test, y_test, test_size=0.25, random_state=42)\n",
    "#We did the standardization before splitting the data to ensure that exactly the same transformation is applied to both dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "In this part, we are only interested in maximizing the **train score**, i.e., having the network memorize the training examples as well as possible.\n",
    "\n",
    "- Propose a parameterization of the network (shape and learning parameters) that will maximize the train score (without considering the test score).\n",
    "\n",
    "While doing this, you should (1) remain within two minutes of training time, and (2) obtain a score that is greater than 0.90.\n",
    "\n",
    "- Is the **test** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "- Explain how the parameters you chose allow the learned model to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default parameters\n",
      "Train score:  0.8452851538383562\n",
      "Test score:   0.7375328102591283 \n",
      "\n",
      "increasing max_iter\n",
      "Train score:  0.8452851538383562\n",
      "Test score:   0.7375328102591283 \n",
      "\n",
      "increasing n_iter_no_change\n",
      "Train score:  0.9076256338554253\n",
      "Test score:   0.7485330155057166 \n",
      "\n",
      "icreasing hidden_layer_sizes\n",
      "Train score:  0.9054794103484964\n",
      "Test score:   0.7658520507413025 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Default parameters\n",
    "mlp = MLPRegressor(max_iter=5000, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"default parameters\")\n",
    "print('Train score: ', mlp.score(X_train, y_train))\n",
    "print('Test score:  ', mlp.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "\n",
    "#Increasing the max iterations number\n",
    "# -> still the same score as the algorithm stops before reaching this max number of iterations\n",
    "mlp = MLPRegressor(max_iter=10000, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"increasing max_iter\")\n",
    "print('Train score: ', mlp.score(X_train, y_train))\n",
    "print('Test score:  ', mlp.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "\n",
    "#Increasing the number of iterations with no change (from default 10 to 100)\n",
    "# -> The score increases too\n",
    "mlp = MLPRegressor(max_iter=5000, random_state=1, n_iter_no_change=100)\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"increasing n_iter_no_change\")\n",
    "print('Train score: ', mlp.score(X_train, y_train))\n",
    "print('Test score:  ', mlp.score(X_test, y_test), \"\\n\")\n",
    "\n",
    "\n",
    "#Adding layers and neurons in each layer\n",
    "mlp = MLPRegressor(max_iter=5000, random_state=1, hidden_layer_sizes=(500,800))\n",
    "mlp.fit(X_train, y_train)\n",
    "print(\"icreasing hidden_layer_sizes\")\n",
    "print('Train score: ', mlp.score(X_train, y_train))\n",
    "print('Test score:  ', mlp.score(X_test, y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion  \n",
    "We can see that increasing n_iter_no_change or hidden_layer_sizes allow us to have a train score > 0.9. But when we increase the hidden_layer_sizes it takes more times to execute. \n",
    "\n",
    "**Definitions** : Overfitting means that *\"our model doesn’t generalize well from our training data to unseen data\"*.\n",
    "\n",
    "**Answer to the questions :**\n",
    "- Is the **test** score substantially smaller than the **train** score (indicator of overfitting) ?  \n",
    "<span style=\"color:blue\">Our test score is always smaller than our train score. The test score is still between 0.75 and 0.78 even when we maximize the train score. It seems logical to have a better score with the set we used for training than for the test set.</span>\n",
    "- Explain how the parameters you chose allow the learned model to overfit.  \n",
    "<span style=\"color:blue\"> Here, the parameters we choose allow the learned model to overfit because we just change when the algorithm will stop or the size of hidden layers. For example, when we add layers and neurons (when we increase the hidden_layer_sizes) we enable the model to learn more, but in that way the model learn also the noise. But when a model do not distinct clearly the noise from the signal, it can provide good result on the test score. However, a test score of 0.77 seems great. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **test** score.\n",
    "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
    "\n",
    "Parameters to vary:\n",
    "\n",
    "- number and size of the hidden layers\n",
    "- activation function\n",
    "- stopping conditions\n",
    "- maximum number of iterations\n",
    "- initial learning rate value\n",
    "\n",
    "Results to present for the tested configurations:\n",
    "\n",
    "- Train/test score\n",
    "- training time\n",
    "\n",
    "\n",
    "Present in a table the various parameters tested and the associated results. You can find in the last cell of the notebook a code snippet that will allow you to plot tables from python structure.\n",
    "Be methodical in the way your run your experiments and collect data. For each run, you should record the parameters and results into an external data structure.\n",
    "\n",
    "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions \n",
    " <span style=\"color:blue\"> *from sklearn.neural_network.MLPRegressor documentation*   </span>   \n",
    "\n",
    "<span style=\"color:blue\">\n",
    "\n",
    "- number and size of the hidden layers    \n",
    "**hidden_layer_sizes** : The ith element represents the number of neurons in the ith hidden layer.  \n",
    "\n",
    "- activation function  \n",
    "**activation** : Activation function for the hidden layer.  \n",
    "\n",
    "- stopping conditions  \n",
    "**early_stopping** : Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as **validation** and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs. Only effective when solver=’sgd’ or ‘adam’.  \n",
    "**validation_fraction** : The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True.  \n",
    "**n_iter_no_change** : Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’. \n",
    "\n",
    "- maximum number of iterations  \n",
    "**max_iter** : Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.  \n",
    "\n",
    "- initial learning rate value  \n",
    "**learning_rate_init** : The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morga\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>validation_fraction</th>\n",
       "      <th>n_iter_no_change</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>validation_score</th>\n",
       "      <th>training time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.816178</td>\n",
       "      <td>3.898172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.815674</td>\n",
       "      <td>4.521911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.809836</td>\n",
       "      <td>39.260190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.804654</td>\n",
       "      <td>3.904051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.795459</td>\n",
       "      <td>134.975118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>(500, 800)</td>\n",
       "      <td>relu</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.782726</td>\n",
       "      <td>164.350574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.779124</td>\n",
       "      <td>5.854262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(500, 800)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.3</td>\n",
       "      <td>10</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.774147</td>\n",
       "      <td>8.340253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>70</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.770092</td>\n",
       "      <td>6.277729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.761622</td>\n",
       "      <td>35.258837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>relu</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>70</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.758616</td>\n",
       "      <td>6.077309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>identity</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.751748</td>\n",
       "      <td>0.805049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>identity</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>70</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.744589</td>\n",
       "      <td>57.986017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(500, 800)</td>\n",
       "      <td>identity</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.744546</td>\n",
       "      <td>13.700360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>identity</td>\n",
       "      <td>False</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.738040</td>\n",
       "      <td>4.703212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>(100,)</td>\n",
       "      <td>identity</td>\n",
       "      <td>True</td>\n",
       "      <td>0.4</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.736671</td>\n",
       "      <td>0.052287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>identity</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.725095</td>\n",
       "      <td>25.070534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>identity</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.719205</td>\n",
       "      <td>4.153771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>identity</td>\n",
       "      <td>False</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.718181</td>\n",
       "      <td>66.746571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>(500, 800)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.5</td>\n",
       "      <td>40</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.666282</td>\n",
       "      <td>26.366779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.665738</td>\n",
       "      <td>20.891040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>(250, 400)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>True</td>\n",
       "      <td>0.2</td>\n",
       "      <td>10</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.531733</td>\n",
       "      <td>1.849090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>False</td>\n",
       "      <td>0.2</td>\n",
       "      <td>70</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.321075</td>\n",
       "      <td>220.044901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>True</td>\n",
       "      <td>0.3</td>\n",
       "      <td>40</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.281901</td>\n",
       "      <td>70.414792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>(750, 1200)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>False</td>\n",
       "      <td>0.3</td>\n",
       "      <td>100</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.100</td>\n",
       "      <td>-0.786426</td>\n",
       "      <td>1117.382203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hidden_layer_sizes activation  early_stopping  validation_fraction  \\\n",
       "17             (100,)       tanh           False                  0.1   \n",
       "13         (250, 400)       relu            True                  0.5   \n",
       "2          (250, 400)       relu           False                  0.5   \n",
       "4          (250, 400)       relu            True                  0.5   \n",
       "10         (250, 400)   logistic           False                  0.5   \n",
       "12         (500, 800)       relu           False                  0.3   \n",
       "22         (250, 400)   logistic           False                  0.3   \n",
       "1          (500, 800)       tanh            True                  0.3   \n",
       "16         (250, 400)       relu            True                  0.5   \n",
       "15         (250, 400)   logistic           False                  0.1   \n",
       "7          (250, 400)       relu            True                  0.5   \n",
       "3              (100,)   identity            True                  0.4   \n",
       "6         (750, 1200)   identity           False                  0.3   \n",
       "5          (500, 800)   identity            True                  0.4   \n",
       "21         (250, 400)   identity           False                  0.1   \n",
       "18             (100,)   identity            True                  0.4   \n",
       "23        (750, 1200)   identity            True                  0.5   \n",
       "0          (250, 400)   identity           False                  0.3   \n",
       "11        (750, 1200)   identity           False                  0.5   \n",
       "8          (500, 800)   logistic            True                  0.5   \n",
       "9          (250, 400)       tanh           False                  0.3   \n",
       "14         (250, 400)   logistic            True                  0.2   \n",
       "24        (750, 1200)   logistic           False                  0.2   \n",
       "19        (750, 1200)       tanh            True                  0.3   \n",
       "20        (750, 1200)       tanh           False                  0.3   \n",
       "\n",
       "    n_iter_no_change  max_iter  learning_rate_init  validation_score  \\\n",
       "17               100      1000               0.001          0.816178   \n",
       "13               100      2500               0.010          0.815674   \n",
       "2                 40      1000               0.100          0.809836   \n",
       "4                 40      2500               0.010          0.804654   \n",
       "10               100      5000               0.005          0.795459   \n",
       "12                40      5000               0.001          0.782726   \n",
       "22                10      2500               0.010          0.779124   \n",
       "1                 10      5000               0.005          0.774147   \n",
       "16                70      5000               0.100          0.770092   \n",
       "15                40      1000               0.001          0.761622   \n",
       "7                 70      2500               0.001          0.758616   \n",
       "3                100      1000               0.010          0.751748   \n",
       "6                 70      1000               0.010          0.744589   \n",
       "5                 40      1000               0.010          0.744546   \n",
       "21               100      2500               0.005          0.738040   \n",
       "18                10      1000               0.010          0.736671   \n",
       "23                40      5000               0.010          0.725095   \n",
       "0                 40      5000               0.100          0.719205   \n",
       "11                40      1000               0.001          0.718181   \n",
       "8                 40      5000               0.100          0.666282   \n",
       "9                 40      1000               0.010          0.665738   \n",
       "14                10      2500               0.100          0.531733   \n",
       "24                70      5000               0.100          0.321075   \n",
       "19                40      1000               0.100          0.281901   \n",
       "20               100      2500               0.100         -0.786426   \n",
       "\n",
       "    training time  \n",
       "17       3.898172  \n",
       "13       4.521911  \n",
       "2       39.260190  \n",
       "4        3.904051  \n",
       "10     134.975118  \n",
       "12     164.350574  \n",
       "22       5.854262  \n",
       "1        8.340253  \n",
       "16       6.277729  \n",
       "15      35.258837  \n",
       "7        6.077309  \n",
       "3        0.805049  \n",
       "6       57.986017  \n",
       "5       13.700360  \n",
       "21       4.703212  \n",
       "18       0.052287  \n",
       "23      25.070534  \n",
       "0        4.153771  \n",
       "11      66.746571  \n",
       "8       26.366779  \n",
       "9       20.891040  \n",
       "14       1.849090  \n",
       "24     220.044901  \n",
       "19      70.414792  \n",
       "20    1117.382203  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random \n",
    "\n",
    "hidden_layer_sizes=[(100,),(250,400),(500,800),(750,1200)]\n",
    "activation=['identity', 'logistic', 'tanh', 'relu']\n",
    "early_stopping =[False,True]\n",
    "validation_fonction=[0.1,0.2,0.3,0.4,0.5]\n",
    "n_iter_no_change=[10,40,70,100]\n",
    "max_iter=[1000,2500,5000]\n",
    "learning_rate_init=[0.001,0.005,0.01,0.1]\n",
    "data = []\n",
    "\n",
    "#TOO MUCH TIME for all the possibilities !!!!\n",
    "#for i in range(len(hidden_layer_sizes)): \n",
    "#    for j in range(len(activation)): \n",
    "#        for k in range(len(early_stopping)): \n",
    "#            for l in range(len(validation_fonction)): \n",
    "#                for m in range(len(n_iter_no_change)): \n",
    "#                    for n in range(len(max_iter)):\n",
    "#                        for o in range(len(learning_rate_init)): \n",
    "\n",
    "for test in range(25):\n",
    "    i=random.choice(hidden_layer_sizes)\n",
    "    j=random.choice(activation)\n",
    "    k=random.choice(early_stopping)\n",
    "    l=random.choice(validation_fonction)\n",
    "    m=random.choice(n_iter_no_change)\n",
    "    n=random.choice(max_iter)\n",
    "    o=random.choice(learning_rate_init)\n",
    "    mlp = MLPRegressor(hidden_layer_sizes=i,\n",
    "                       activation= j,\n",
    "                        early_stopping = k,\n",
    "                        validation_fraction=l,\n",
    "                        n_iter_no_change=m,\n",
    "                        max_iter= n, \n",
    "                        learning_rate_init=o            \n",
    "                        )\n",
    "    \n",
    "    start = time.time()\n",
    "    mlp.fit(X_train, y_train)\n",
    "    stop = time.time()\n",
    "                            \n",
    "    data.append({'hidden_layer_sizes':i,\n",
    "                 'activation':j,\n",
    "                 'early_stopping': k, \n",
    "                 'validation_fraction':l,\n",
    "                 'n_iter_no_change':m, \n",
    "                 'max_iter': n, \n",
    "                 'learning_rate_init':o, \n",
    "                 'validation_score': mlp.score(X_validation, y_validation),\n",
    "                 'training time': stop - start\n",
    "                })\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.replace(np.nan, '-')\n",
    "table = table.sort_values(by='validation_score', ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "\n",
    "Unless you used cross-validation, you have probably used the \"test\" set to select the best model among the ones you experimented with.\n",
    "Since your model is the one that worked best on the \"test\" set, your selection is *biased*.\n",
    "\n",
    "In all rigor the original dataset should be split in three:\n",
    "\n",
    "- the **training set**, on which each model is trained\n",
    "- the **validation set**, that is used to pick the best parameters of the model \n",
    "- the **test set**, on which we evaluate the final model\n",
    "\n",
    "\n",
    "Evaluate the score of your algorithm on a test set that was not used for training nor for model selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "<span style=\"color:blue\">\n",
    "We already split our data set on 3 to have a training set, a validation set and a test set.   \n",
    "25% of our initial dataset constitute the test set. And 25% of our train dataset constitute the validation test (0.1875 of the initial dataset). So the train dataset represent 0.5625 of the initial dataset.\n",
    "\n",
    "From our experiments the best model seems to be :\n",
    "```\n",
    "MLPRegressor(hidden_layer_sizes=(100,),\n",
    "                       activation= tanh,\n",
    "                        early_stopping = False,\n",
    "                        validation_fraction=0.1,\n",
    "                        n_iter_no_change=100,\n",
    "                        max_iter=1000 , \n",
    "                        learning_rate_init=0.001            \n",
    "                        )\n",
    "```\n",
    "With a validation score = 0.816178.  \n",
    "We will now evalutate our model with the test set.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\n",
      "Test score:   0.829903487210372 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morga\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPRegressor(hidden_layer_sizes=(100,),\n",
    "                        activation= 'tanh',\n",
    "                        early_stopping = False,\n",
    "                        validation_fraction=0.1,\n",
    "                        n_iter_no_change=100,\n",
    "                        max_iter=1000 , \n",
    "                        learning_rate_init=0.001            \n",
    "                        )\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(X_train, y_train)\n",
    "stop = time.time()\n",
    "\n",
    "print(\"Result\")\n",
    "print('Test score:  ', mlp.score(X_test, y_test), \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "<span style=\"color:blue\"> We have a final test score at 0.8299 that is predictably less than previously but it still a very good score ! <span style=\"color:blue\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
